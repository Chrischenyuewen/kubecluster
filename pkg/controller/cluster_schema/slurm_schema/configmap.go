package slurm_schema

import (
	"context"
	"fmt"
	kubeclusterorgv1alpha1 "github.com/chriskery/kubecluster/apis/kubecluster.org/v1alpha1"
	"github.com/chriskery/kubecluster/pkg/common"
	"github.com/chriskery/kubecluster/pkg/util"
	utillabels "github.com/chriskery/kubecluster/pkg/util/labels"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"strconv"
	"strings"
)

const (
	placeHolderClusterName    = "${CLUSTER_NAME}"
	placeHolderControlMachine = "${CONTROL_MACHINE}"
	placeHolderControlAddr    = "${CONTROL_ADDRESS}"
	placeHolderSlurmctldPort  = "${SLURMCTLD_PORT}"
	placeHolderSlurmdPort     = "${slurmd_port}"
)

const (
	spoolDir = "/var/spool"
)

var (
	slurmConfTemplate = "#\n# slurm.conf file. Please run configurator.html\n# (in doc/html) to build a configuration file customized\n# for your environment.\n#\n#\n# slurm.conf file generated by configurator.html.\n# Put this file on all nodes of your cluster.\n# See the slurm.conf man page for more information.\n#\n################################################\n#                   CONTROL                    #\n################################################\n" +
		fmt.Sprintf("ClusterName=%s \n", placeHolderClusterName) +
		fmt.Sprintf("ControlMachine=%s \n", placeHolderControlMachine) +
		fmt.Sprintf("ControlAddr=%s \n", placeHolderControlAddr) +
		"\n################################################\n#            LOGGING & OTHER PATHS             #\n################################################\n" +
		"SlurmctldPidFile=/var/run/slurmctld.pid \n" +
		"SlurmdPidFile=/var/run/slurmd.pid \n" +
		"SlurmctldDebug=3 \n" +
		"SlurmdDebug=3 \n" +
		fmt.Sprintf("SlurmctldPort=%s \n", placeHolderSlurmctldPort) +
		fmt.Sprintf("SlurmdPort=%s \n", placeHolderSlurmdPort) +
		"SlurmdSpoolDir=" + spoolDir + "/slurmd \n" +
		"SlurmdUser=root \n" +
		"StateSaveLocation=" + spoolDir + " \n" +
		"\n################################################\n#           SCHEDULING & ALLOCATION            #\n################################################\n" +
		"SchedulerType=sched/backfill \n" +
		"SelectType=select/cons_tres \n" +
		"SelectTypeParameters=CR_Core_Memory \n" +
		"\n################################################\n#                    OTHER                     #\n################################################\n" +
		"MpiDefault=none \n" +
		"SwitchType=switch/none \n" +
		"ProctrackType=proctrack/pgid \n" +
		"ReturnToService=1 \n" +
		"TaskPlugin=task/none \n" +
		"################################################\n#                  ACCOUNTING                  #\n################################################\n" +
		"AccountingStorageType=accounting_storage/none \n" +
		"jobAcctGatherType=jobacct_gather/none \n" +
		"GresTypes=gpu \n"
	resourceTypeGpu = "gpu"
)

func (s *slurmClusterSchemaReconciler) getHostNetWorkNodeList(kcluster *kubeclusterorgv1alpha1.KubeCluster) []string {
	var hostNetWorkNodeList []string
	for replicaType, spec := range kcluster.Spec.ClusterReplicaSpec {
		if !isHostNetWork(spec) || spec.Replicas == nil {
			continue
		} else {
			for i := 0; i < int(*spec.Replicas); i++ {
				hostNetWorkNodeList = append(hostNetWorkNodeList, common.GenGeneralName(kcluster.Name, replicaType, strconv.Itoa(i)))
			}
		}
	}
	return hostNetWorkNodeList
}

const entrypointShellTemplate = `#!/bin/bash

addUserIfNotExist(){
  if id -u "$1" >/dev/null 2>&1; then
     echo "$1 exists"
  else
     echo "create user $1"
     useradd "$1"
  fi
}
addUserIfNotExist slurm
addUserIfNotExist munge

mkdir -p /etc/munge
chown  munge:munge /etc/munge
# shellcheck disable=SC2024

##cp  relative cmd

# shellcheck disable=SC2162
read -p "please input slurm relative cmd source dir :" cmddir

if [ ! -d "$cmddir" ]; then
   echo "$cmddir not exist, skip cp  slurm relative cmd"
   exit 0;
fi


cp  -p -L "$cmddir"/slurm/bin/* /usr/bin
cp  -p -L "$cmddir"/slurm/sbin/* /usr/sbin
cp  -p -L "$cmddir"/munge/bin/* /usr/bin
cp  -p -L "$cmddir"/munge/sbin/* /usr/sbin
if [ ! -e /etc/munge/munge.key ];then
  cp  -p -L "$cmddir"/munge/munge/munge.key /etc/munge/munge.key
fi
#dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key

# shellcheck disable=SC2024
echo "$cmddir/slurm/lib"  >> /etc/ld.so.conf.d/slurm-x86_64.conf
# shellcheck disable=SC2024
echo "$cmddir/slurm/lib/slurm" >> /etc/ld.so.conf.d/slurm-x86_64.conf
# shellcheck disable=SC2024
echo "$cmddir/munge/lib" >>  /etc/ld.so.conf.d/munge-x86_64.conf

ldconfig

chown munge:munge /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
mkdir -p /var/lib/munge
mkdir -p /var/run/munge
mkdir -p /var/log/munge
chown -R munge:munge /var/lib/munge
chown -R munge:munge /var/run/munge
chown -R munge:munge /var/log/munge

echo "configure completed"`

func (s *slurmClusterSchemaReconciler) ReconcileConfigMap(kcluster *kubeclusterorgv1alpha1.KubeCluster, configMap *corev1.ConfigMap) error {
	slurmConf, exists := configMap.Data[slurmConfKey]
	if exists {
		slurmctlPort, slurmdPort, err := getSlurmPortsFromConf(slurmConf)
		if err != nil {
			return err
		}
		s.setSlurmPorts(kcluster, slurmctlPort, slurmdPort)
	}

	needReconcile := s.needReconcileConfigMap(configMap)
	if exists && !needReconcile {
		return nil
	}

	if configMap.Data == nil {
		configMap.Data = make(map[string]string)
	}
	if configMap.BinaryData == nil {
		configMap.BinaryData = make(map[string][]byte)
	}
	_, exists = configMap.Data[controllerEntrypoint]
	if !exists {
		configMap.Data[controllerEntrypoint] = s.genControllerEntrypoint()
	}
	_, exists = configMap.Data[workerEntrypoint]
	if !exists {
		configMap.Data[workerEntrypoint] = s.genWorkerEntrypoint()
	}
	_, exists = configMap.Data[slurmConfKey]
	if !exists {
		slurmConf, err := s.genSlurmConf(kcluster)
		if err != nil {
			return err
		}
		configMap.Data[slurmConfKey] = slurmConf
	}

	hostNetWorkNodeList := s.getHostNetWorkNodeList(kcluster)
	if len(hostNetWorkNodeList) == 0 {
		configMap.Data[configMapReadyKey] = "true"
	} else {
		configMap.Data[configMapReadyKey] = "false"
		configMap.Data[hostNetWorkNodesListKey] = strings.Join(hostNetWorkNodeList, ",")
	}
	_, exists = configMap.Data[mungeKey]
	if !exists {
		configMap.BinaryData[mungeKey] = genMungeKey()
	}
	return nil
}

func genMungeKey() []byte {
	return []byte(util.RandStr(mungeKeylenInInt))
}

func (s *slurmClusterSchemaReconciler) needReconcileConfigMap(configMap *corev1.ConfigMap) bool {
	cmReady, exists := configMap.Data[configMapReadyKey]
	if exists || cmReady == "true" {
		return false
	} else {
		return true
	}
}

func (s *slurmClusterSchemaReconciler) createSlurmConfPartitions(_ *kubeclusterorgv1alpha1.KubeCluster) string {
	partitions := "################################################\n#                  PARTITIONS                  #\n################################################\n"
	partitions += "PartitionName=all Nodes=ALL Default=YES MaxTime=INFINITE State=UP\n"
	return partitions
}

func (s *slurmClusterSchemaReconciler) createSlurmConfNodesByReplica(
	clusterName string,
	replicaType kubeclusterorgv1alpha1.ReplicaType,
	spec *kubeclusterorgv1alpha1.ReplicaSpec,
) (string, error) {
	var tempConfig string
	if spec.Replicas == nil {
		return "", nil
	}
	replicaResourceList := getResourceRequestList(spec)
	cpus, realMemory, gpus, err := getResourceRequest(replicaResourceList)
	if err != nil {
		return "", err
	}
	for i := 0; i < int(*spec.Replicas); i++ {
		generalName := common.GenGeneralName(clusterName, replicaType, strconv.Itoa(i))
		tempConfig += "NodeName=" + generalName + " " +
			"NodeAddr=" + generalName + " " +
			"CPUs=" + strconv.FormatInt(cpus, 10) + " " +
			"RealMemory=" + strconv.FormatInt(realMemory, 10) + " " +
			//TODO(Chris):Add Gpu Type
			"Gres=gpu:" + strconv.FormatInt(gpus, 10) + " " +
			"State=UNKNOWN \n"
	}
	return tempConfig, nil
}

func (s *slurmClusterSchemaReconciler) createSlurmConfNodes(kcluster *kubeclusterorgv1alpha1.KubeCluster) (string, error) {
	nodeConf := strings.Builder{}
	var controllerConf, replicaConf string
	var err error
	for replicaType, replicaSpec := range kcluster.Spec.ClusterReplicaSpec {
		if replicaType == SchemaReplicaTypeController {
			//TODO:shound we need controller to participate computing?
			//controllerConf, err = s.createSlurmConfNodesByReplica(kcluster.Name, replicaType, replicaSpec)
			//if err != nil {
			//	return "", err
			//}
		} else {
			replicaConf, err = s.createSlurmConfNodesByReplica(kcluster.Name, replicaType, replicaSpec)
			if err != nil {
				return "", err
			}
			nodeConf.WriteString(replicaConf)
		}
	}
	var nodePrefix = "################################################\n#                    NODES                     #\n################################################\n"
	return fmt.Sprintf("%s%s\n%s", nodePrefix, controllerConf, nodeConf.String()), nil
}

func (s *slurmClusterSchemaReconciler) createSlurmConf(
	kcluster *kubeclusterorgv1alpha1.KubeCluster,
	masterPort int,
	workerPort int,
) (string, error) {
	tempConfig := slurmConfTemplate
	tempConfig = strings.Replace(tempConfig, placeHolderClusterName, kcluster.Name, 1)
	tempConfig = strings.Replace(tempConfig, placeHolderControlMachine,
		common.GenGeneralName(kcluster.Name, SchemaReplicaTypeController, strconv.Itoa(0)), 1)
	tempConfig = strings.Replace(tempConfig, placeHolderControlAddr,
		common.GenGeneralName(kcluster.Name, SchemaReplicaTypeController, strconv.Itoa(0)), 1)
	tempConfig = strings.Replace(tempConfig, placeHolderSlurmctldPort, strconv.Itoa(masterPort), 1)
	tempConfig = strings.Replace(tempConfig, placeHolderSlurmdPort, strconv.Itoa(workerPort), 1)

	nodesConf, err := s.createSlurmConfNodes(kcluster)
	if err != nil {
		return "", err
	}
	partitionsConf := s.createSlurmConfPartitions(kcluster)

	return fmt.Sprintf("%s\n%s\n%s\n", tempConfig, nodesConf, partitionsConf), nil
}

const (
	slurmConfKey            = "slurm.conf"
	mungeKey                = "munge.key"
	mungeKeylenInInt        = 1024
	configMapReadyKey       = "ready"
	configMapReady          = "true"
	configMapReadyFile      = "/tmp/configmap-ready"
	hostNetWorkNodesListKey = "hostNetWorkNodesList"
	controllerEntrypoint    = "controller-entrypoint"
	workerEntrypoint        = "worker-entrypoint"
)

func (s *slurmClusterSchemaReconciler) UpdateConfigMap(kcluster *kubeclusterorgv1alpha1.KubeCluster, configMap *corev1.ConfigMap) error {
	cmReady, exist := configMap.Data[configMapReadyKey]
	if exist || cmReady == configMapReady {
		return nil
	}

	hostNetWorkNodesList := configMap.Data[hostNetWorkNodesListKey]
	if len(hostNetWorkNodesList) == 0 {
		configMap.Data[configMapReadyKey] = configMapReady
		return nil
	}

	var filter common.ObjectFilterFunction = func(obj metav1.Object) bool {
		return metav1.IsControlledBy(obj, kcluster)
	}
	hostNetWorkNodesListSet := sets.New[string](strings.Split(hostNetWorkNodesList, ",")...)
	slurmConf := configMap.Data[slurmConfKey]
	for replicaType, spec := range kcluster.Spec.ClusterReplicaSpec {
		if hostNetWorkNodesListSet.Len() == 0 {
			break
		}
		if !isHostNetWork(spec) {
			continue
		}

		labels := utillabels.GenLabels(common.ControllerName, kcluster.GetName())
		utillabels.SetReplicaType(labels, string(replicaType))
		utillabels.SetClusterType(labels, string(kcluster.Spec.ClusterType))
		// Create selector.
		selector, err := metav1.LabelSelectorAsSelector(&metav1.LabelSelector{
			MatchLabels: labels,
		})
		if err != nil {
			return fmt.Errorf("couldn't convert Cluster selector: %v", err)
		}
		// List all pods to include those that don't match the selector anymore
		// but have a ControllerRef pointing to this controller.
		podlist := &corev1.PodList{}
		err = s.List(context.Background(), podlist,
			client.MatchingLabelsSelector{Selector: selector}, client.InNamespace(kcluster.GetNamespace()))
		if err != nil {
			return err
		}

		replicaPodList := common.ConvertPodListWithFilter(podlist.Items, filter)
		for _, pod := range replicaPodList {
			if !hostNetWorkNodesListSet.Has(pod.GetName()) {
				continue
			}
			if len(pod.Spec.NodeName) == 0 || len(pod.Spec.Hostname) == 0 {
				continue
			}
			hostName := pod.Spec.Hostname
			if len(hostName) == 0 {
				hostName = pod.Spec.NodeName
			}
			slurmConf = strings.Replace(slurmConf, "NodeName="+pod.GetName(), "NodeName="+hostName, 1)
			hostNetWorkNodesListSet.Delete(pod.GetName())
		}
	}

	configMap.Data[slurmConfKey] = slurmConf
	if hostNetWorkNodesListSet.Len() == 0 {
		configMap.Data[hostNetWorkNodesListKey] = ""
		configMap.Data[configMapReadyKey] = configMapReady
	} else {
		configMap.Data[hostNetWorkNodesListKey] = strings.Join(hostNetWorkNodesListSet.UnsortedList(), ",")
		configMap.Data[configMapReadyKey] = configMapReady
	}
	return nil
}

func (s *slurmClusterSchemaReconciler) genControllerEntrypoint() string {
	genGrepCommand := ">> /tmp/gres.conf && for (( i=0; i < 8; i++ )) do if [ -a /dev/nvidia${i} ]; then echo \"Name=gpu Type=%s File=/dev/nvidia${i}\" >> /tmp/gres.conf; fi ; done"
	//TODO:(support gpu type)
	genGrepCommand = fmt.Sprintf(genGrepCommand, "gpu")
	cpGrepsCommand := fmt.Sprintf("cp /tmp/gres.conf %s ", SlurmConfDir)

	entrypointShell := fmt.Sprintf("%s\n%s\n", entrypointShellTemplate, strings.Join([]string{genGrepCommand, cpGrepsCommand}, " && "))
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, "sleep 5")

	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, fmt.Sprintf("echo \"clear spool-cluster-name: %s/clustername \" && rm -rf %s/clustername", spoolDir, spoolDir))
	mungedCMd := fmt.Sprintf("echo \"starting munged\" && munged -f --key-file %s --log-file %s --pid-file %s --seed-file %s --socket %s && echo \"munged started\" ",
		"/etc/munge/munge.key", "/var/log/munge/munged.log", "/var/run/munge/munged.pid", "/var/lib/munge/munge.seed", "/var/run/munge/munge.socket.2")
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, mungedCMd)
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, "echo \"starting slurmctld\" && slurmctld -D && echo \"slurmctld started\"")

	//TODO:shound we need controller to participate computing?
	//cmds = append(cmds, "echo \"starting slurmd\" && slurmd -D ")
	return entrypointShell
}

func (s *slurmClusterSchemaReconciler) genWorkerEntrypoint() string {
	genGrepCommand := ">> /tmp/gres.conf && for (( i=0; i < 8; i++ )) do if [ -a /dev/nvidia${i} ]; then echo \"Name=gpu Type=%s File=/dev/nvidia${i}\" >> /tmp/gres.conf; fi ; done"
	//TODO:(support gpu type)
	genGrepCommand = fmt.Sprintf(genGrepCommand, "gpu")
	cpGrepsCommand := fmt.Sprintf("cp /tmp/gres.conf %s ", SlurmConfDir)

	entrypointShell := fmt.Sprintf("%s\n%s\n", entrypointShellTemplate, strings.Join([]string{genGrepCommand, cpGrepsCommand}, " && "))
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, "sleep 30")

	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, fmt.Sprintf("echo \"clear spool-cluster-name: %s/clustername \" && rm -rf %s/clustername", spoolDir, spoolDir))
	mungedCMd := fmt.Sprintf("echo \"starting munged\" && munged -f --key-file %s --log-file %s --pid-file %s --seed-file %s --socket %s && echo \"munged started\" ",
		"/etc/munge/munge.key", "/var/log/munge/munged.log", "/var/run/munge/munged.pid", "/var/lib/munge/munge.seed", "/var/run/munge/munge.socket.2")
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, mungedCMd)
	entrypointShell = fmt.Sprintf("%s\n%s\n", entrypointShell, "echo \"starting slurmd\" && slurmd -D ")

	return entrypointShell

}

func (s *slurmClusterSchemaReconciler) genSlurmConf(kcluster *kubeclusterorgv1alpha1.KubeCluster) (string, error) {
	slurmctlPort, slurmdPort, err := getSlurmPortsFromSpec(kcluster, s.GetDefaultContainerName())
	if err != nil {
		slurmctlPort, slurmdPort = genSlurmRandomPort()
		s.Recorder.Eventf(kcluster, corev1.EventTypeNormal, "SlurmPortsNotSet", fmt.Sprintf("Will use defaul slurmctld's port:%d, slurmd's port : %d", slurmctlPort, slurmdPort))
	}
	s.setSlurmPorts(kcluster, slurmctlPort, slurmdPort)

	slurmConf, err := s.createSlurmConf(kcluster, slurmctlPort, slurmdPort)
	if err != nil {
		return "", fmt.Errorf("generateSlurmConfig err:%v", err)
	}
	return slurmConf, nil
}
